# Task 2: Data Engineering и визуализация с RAPIDS

## Часть 1: Форматы данных, pandas, cuDF и Dask

### Форматы данных

#### JSON (JavaScript Object Notation)
- Текстовый формат, удобный для веб-API
- Человекочитаемый, но не компактный — имена полей повторяются для каждой записи
- Хорошо подходит для передачи данных между сервисами, но не для хранения больших объёмов

#### CSV (Comma Separated Values)
- Текстовый формат, разделённый запятыми
- Человекочитаемый, знаком по Excel и Google Sheets
- Оптимизирован для строк (row-oriented)
- Может содержать строку заголовка с именами столбцов
- Не обеспечивает сжатие

#### Parquet (Apache Parquet)
- Бинарный формат — не человекочитаемый
- **Оптимизирован для столбцов** (column-oriented)
- Содержит метаданные (header и footer) с описанием схемы
- Столбцы разбиты на чанки — программа читает метаданные, чтобы найти нужные столбцы
- Значительно компактнее CSV (в тестах: CSV ~1.1 ГБ → Parquet ~522 МБ для тех же данных)
- Быстрее при чтении с фильтрацией столбцов

#### ZIP-сжатие
- Кодирует избыточные данные в меньшее количество байт
- pandas и cuDF могут читать сжатые CSV напрямую через параметр `compression="zip"`

### pandas vs cuDF

**pandas** — широко используемая библиотека для работы с DataFrame на **CPU**:
- Чтение/запись: CSV, Excel, JSON, SQL и другие форматы
- Очистка данных: обработка пропущенных значений, слияние, изменение формы
- Анализ: группировка, агрегация, статистические операции

**cuDF** — GPU-ускоренный аналог pandas из экосистемы **RAPIDS**:
- Аналогичный pandas API
- Использует параллельную обработку на GPU NVIDIA
- Значительное ускорение для больших датасетов

Пример сравнения скорости чтения CSV (7500×7500 числовой датасет):
- pandas: ~16.9 сек
- cuDF: ~1.4 сек (в 12 раз быстрее)

Для Parquet:
- pandas: ~2.4 сек
- cuDF: ~1.4 сек

**Когда cuDF быстрее:** большие датасеты с числовыми данными. Для очень маленьких данных накладные расходы GPU могут свести на нет преимущества.

### Dask — параллельные вычисления

**Dask** — библиотека для параллелизации Python-кода. Она является device-agnostic и масштабируется как на CPU, так и на GPU.

Ключевые компоненты Dask:
- **Dask DataFrame** — распределённый аналог pandas DataFrame
- **Dask Array** — распределённый аналог NumPy ndarray
- **Dask Bag** — набор с поддержкой дубликатов и разнотипных данных

#### Принцип работы Dask: DAG (Directed Acyclic Graph)

Когда приложение использует Dask API, оно строит направленный ациклический граф (DAG) задач. DAG определяет **что** нужно вычислить, но **не выполняет** вычисления сразу.

Три этапа:
1. Построение DAG задач
2. Планирование и выполнение DAG
3. (Опционально) Запуск выделенных процессов worker и scheduler для распределённого выполнения

#### Ленивые вычисления

При вызове `dask_cudf.read_csv()` данные **не считываются** — создаётся только граф операций. Для получения результата нужно вызвать `.compute()`.

```python
ddf = dask_cudf.read_csv("data/*.csv")  # только построение графа
result = ddf.compute()                   # реальное чтение и выполнение
```

Dask эффективен, когда данные разбиты на **множество файлов** — он может читать их параллельно. Для одного файла pandas/cuDF могут быть быстрее из-за накладных расходов на построение DAG.

### MapReduce

Два основных типа параллельных операций:
- **Map** — функция, выполняемая каждым worker'ом независимо (например, `+10` к каждому значению). Может работать асинхронно.
- **Reduce** — функция, требующая агрегации данных от всех worker'ов (например, `sum`, `mean`). Вызывает синхронизацию.

MapReduce — комбинация этих операций:
```python
# Map → Reduce
result = (ddf["Water Level"] + 10).sum()

# Reduce → Map (другой граф!)
result = ddf["Water Level"].sum() + 10
```

**Порядок операций влияет на граф** и производительность. По возможности следует минимизировать Reduce-операции, чтобы избежать простоя worker'ов.

---

## Часть 2: ETL-пайплайны с NVTabular

### Что такое NVTabular?

**NVTabular** — библиотека для быстрой трансформации и загрузки табличных данных, способная обрабатывать датасеты терабайтного масштаба. Использует GPU через библиотеку RAPIDS cuDF.

### 4 основных компонента NVTabular

1. **Dataset** — содержит список файлов и итерирует по ним, при необходимости читая файл по частям (chunks)
2. **Op** — определяет вычислительную операцию (вычисление среднего, заполнение пропусков, комбинирование категорий и т.д.)
3. **Workflow** — оркестрирует пайплайн, строит DAG с Dask
4. **Dataloader** — оптимизированные загрузчики для PyTorch и TensorFlow

### Процесс создания ETL-пайплайна

**1. Определение ColumnGroup:**
```python
columns = ["STATION", "DATE", "HourlyDryBulbTemperature", ...]
column_group = nvt.ColumnGroup(columns)
```

**2. Инициализация Workflow:**
```python
workflow = nvt.Workflow(column_group)
```

**3. Инициализация Dataset:**
```python
dataset = nvt.Dataset(file_paths)  # поддерживает .csv, .parquet, .avro
```

**4. Трансформация и сохранение:**
```python
workflow.transform(dataset).to_parquet(output_path="data/output/")
```

### Операции (Ops)

Ops применяются к ColumnGroup через оператор `>>`:
```python
features = ["col1", "col2"] >> nvt.ops.Normalize() >> nvt.ops.FillMissing()
```

#### Встроенные операции:
- **Normalize** — нормализация (вычитание среднего, деление на стандартное отклонение)
- **FillMissing** — заполнение пропущенных значений константой
- **FillMedian** — заполнение пропущенных значений медианой
- **DifferenceLag** — вычисление разности между соседними строками
- **Rename** — переименование столбцов (добавление постфикса/префикса)

Параметр `add_binary_cols=True` создаёт дополнительный boolean-столбец, маркирующий изменённые значения.

#### LambdaOp — пользовательские операции:
```python
def fahrenheit_to_celsius(col):
    return (col - 32) * (5 / 9)

celsius_cols = ["Temperature"] >> nvt.ops.LambdaOp(fahrenheit_to_celsius)
```

### Fitting данных

Для операций, требующих статистики (например, Normalize), необходимо вызвать `workflow.fit(dataset)` — аналогично scikit-learn API:

```python
workflow.fit(dataset)     # вычисляет среднее и стандартное отклонение
workflow.transform(dataset).to_parquet(...)  # применяет нормализацию
```

### Multi-GPU с LocalCUDACluster

NVTabular масштабируется на несколько GPU через Dask `LocalCUDACluster`:

```python
from dask_cuda import LocalCUDACluster
from dask.distributed import Client

cluster = LocalCUDACluster(
    protocol="tcp",
    CUDA_VISIBLE_DEVICES="0,1,2,3",
    local_directory="/tmp/",
    device_memory_limit=device_mem_size(kind="total") * 0.9
)
client = Client(cluster)
```

После инициализации кластера NVTabular автоматически использует все доступные GPU.

**Инициализация пулов памяти** на всех worker'ах:
```python
import rmm
def _rmm_pool():
    rmm.reinitialize()
client.run(_rmm_pool)
```

### Мониторинг GPU

Команда `nvidia-smi` для мониторинга:
```bash
watch -n0.1 nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv
```

Показывает: индекс GPU, используемую/общую память, загрузку GPU.

### Интеграция с Dask

NVTabular Dataset можно конвертировать в Dask DataFrame:
```python
dataset = nvt.Dataset(files, engine='parquet', part_size='100MB')
ddf = dataset.to_ddf()
```

Параметры партиционирования:
- `part_size` — размер каждой партиции в байтах (рекомендуется `128MB` или `256MB`)
- `part_mem_fraction` — доля GPU-памяти на партицию (по умолчанию 0.125 = 12.5%)

---

## Часть 3: Исследование данных и визуализация

### Исследование данных (Data Exploration)

Оптимизированные пайплайны строятся через глубокое понимание характеристик данных.

#### Описательная статистика с `.describe()`

Вычисляет для каждого столбца:
- **count** — количество ячеек со значением
- **mean** — среднее
- **std** — стандартное отклонение (мера разброса)
- **min** — минимум
- **25%** — первый квартиль
- **50%** — медиана
- **75%** — третий квартиль
- **max** — максимум

#### Обработка пропущенных значений

Некорректные значения (например, `-9999` как маркер отсутствия) искажают все статистики. Решение — указать их при чтении:
```python
df = cudf.read_csv('data.csv', na_values=['-9999'])
```

#### Гистограммы

Гистограммы показывают распределение данных. Помогают ответить на вопросы:
- Каков диапазон (минимум и максимум)?
- Каков разброс? Равномерное ли распределение?
- Есть ли скошенность (skew)?

### Масштабирование с Dask

Для чтения тысяч файлов одновременно используется `dask_cudf`:
```python
ddf = dask_cudf.read_csv(
    "data/*.csv",
    usecols=["STATION", "LATITUDE", "LONGITUDE", "DlySum", "DATE"],
    dtype={"LATITUDE": np.float32, "LONGITUDE": np.float32, ...},
    na_values=["-9999"]
)
```

**Ключевые параметры:**
- `usecols` — раннее фильтрование столбцов снижает расход памяти
- `dtype` — явное указание типов предотвращает ошибки на редких значениях
- `na_values` — маркеры отсутствующих значений

#### Ленивое выполнение (Lazy Execution)

Dask не выполняет вычисления сразу — он строит граф операций (DAG). Вычисления запускаются через:
- `.compute()` — возвращает результат как обычный DataFrame
- `.persist()` — сохраняет результат в памяти Dask для повторного использования

```python
ddf = ddf.persist()  # данные считаны и хранятся в памяти
result = ddf.describe().compute()  # быстрый доступ
```

### Визуализация больших данных с Plotly

Plotly не может напрямую работать с данными на GPU. Схема работы:

**GPU (cuDF/Dask) → фильтрация/вычисления → CPU (pandas) → Plotly**

Пример карты осадков с `Scattergeo`:
```python
import plotly.graph_objects as go

fig = go.Figure([go.Scattergeo(
    lon=data["LONGITUDE"],
    lat=data["LATITUDE"],
    mode='markers',
    marker_color=data['DlySum'],
    text=data['STATION']
)])
fig.show()
```

### Интерактивные дашборды с Plotly Dash

**Plotly Dash** расширяет Plotly для создания веб-сервисов с интерактивными дашбордами.

Архитектура:
1. Пользователь выбирает параметры (дату, фильтры) на клиенте
2. Параметры отправляются на сервер
3. GPU фильтрует данные
4. Результат передаётся обратно на CPU → Plotly генерирует график → отправляет клиенту

#### Компоненты Dash

- **Layout** — HTML-структура дашборда:
  ```python
  app.layout = html.Div([
      dcc.DatePickerSingle(id='date-picker', ...),
      daq.BooleanSwitch(id='show-zeros', ...),
      dcc.Graph(id='precipitation-map')
  ])
  ```

- **Callbacks** — связывают входы с Python-функцией:
  ```python
  @app.callback(
      dash.dependencies.Output('precipitation-map', 'figure'),
      [dash.dependencies.Input('date-picker', 'date'),
       dash.dependencies.Input('show-zeros', 'on')])
  def update_graph(date_value, show_zeros):
      dff = df[df.index == date_value]
      # ... построение графика ...
      return fig
  ```

- `Output` — элемент, куда записывается результат
- `Input` — элементы, откуда читаются значения
- Функция вызывается при каждом изменении любого `Input`

---

## Вопросы и ответы

**1. В чём ключевое отличие между CSV и Parquet форматами?**

CSV — текстовый формат, оптимизированный для хранения по строкам, человекочитаемый, но не сжатый. Parquet — бинарный колоночный формат с встроенными метаданными и сжатием. Parquet значительно компактнее (примерно в 2 раза меньше CSV для числовых данных) и быстрее при чтении с фильтрацией столбцов, потому что можно прочитать только нужные столбцы без сканирования всего файла.

**2. Чем cuDF отличается от pandas и когда cuDF быстрее?**

pandas выполняет операции на CPU, cuDF — на GPU с использованием NVIDIA CUDA. cuDF обеспечивает значительное ускорение (в 10+ раз) при работе с большими датасетами, где GPU может эффективно распараллелить обработку. Для очень маленьких данных pandas может быть быстрее из-за накладных расходов на передачу данных на GPU. API cuDF максимально совместим с pandas для лёгкого перехода.

**3. Что такое Dask и как он организует параллельные вычисления?**

Dask — библиотека для параллельных вычислений в Python. Она строит DAG (направленный ациклический граф) задач, который затем выполняется планировщиком параллельно. Dask поддерживает DataFrame, Array и Bag структуры данных. Он особенно полезен при работе с данными, которые не помещаются в память одного устройства, распределяя их по нескольким worker'ам.

**4. Что такое ленивые вычисления (lazy evaluation) в Dask?**

При вызове операций Dask не выполняет их сразу — он строит граф вычислений (DAG). Реальные вычисления запускаются только при явном запросе результата: через `.compute()` (возвращает обычный DataFrame) или `.persist()` (сохраняет результат в распределённой памяти Dask). Это позволяет оптимизировать граф до его выполнения.

**5. В чём разница между Map и Reduce операциями в MapReduce?**

Map — операция, которую каждый worker выполняет независимо над своей порцией данных (например, прибавить 10 к каждому значению). Reduce — операция, требующая обмена данными между worker'ами для агрегации (например, вычисление суммы или среднего по всем данным). Map работает асинхронно, а Reduce вызывает синхронизацию. Для максимальной производительности следует минимизировать количество Reduce-операций.

**6. Какие основные компоненты NVTabular и для чего они служат?**

NVTabular имеет 4 компонента: Dataset — абстракция над списком файлов с поддержкой чтения по частям; Op — определяет вычислительную операцию (Normalize, FillMissing, LambdaOp и др.); Workflow — оркестрирует пайплайн и строит DAG; Dataloader — оптимизированные загрузчики данных для PyTorch и TensorFlow. Вместе они позволяют строить масштабируемые ETL-пайплайны на GPU.

**7. Что делает метод `workflow.fit()` в NVTabular и когда он нужен?**

Метод `fit()` вычисляет статистическую информацию по датасету — аналогично scikit-learn API. Он нужен, когда операции требуют знания глобальных статистик: например, Normalize требует средние и стандартные отклонения, FillMedian — медианы. После `fit()` эти статистики сохраняются и могут применяться к новым данным без повторного вычисления.

**8. Как масштабировать NVTabular на несколько GPU?**

Нужно инициализировать `LocalCUDACluster` из `dask_cuda`, указав доступные GPU через `CUDA_VISIBLE_DEVICES`, протокол (tcp или ucx для NVLink), лимит памяти и временную директорию. Затем создать Dask `Client`, подключённый к кластеру. После этого NVTabular автоматически распределяет работу между всеми указанными GPU. Рекомендуется также инициализировать пулы памяти RMM на каждом worker'е.

**9. Как правильно работать с Plotly при наличии данных на GPU?**

Plotly не может напрямую читать данные из GPU-памяти. Рабочий процесс: выполнить фильтрацию и вычисления на GPU (cuDF/Dask cuDF), затем перенести результат на CPU через `.to_pandas()` или `.to_array()`, и передать эти данные в Plotly для визуализации. Такой подход сочетает скорость GPU для вычислений с гибкостью Plotly для отображения.

**10. Как работает интерактивный дашборд Plotly Dash?**

Dash состоит из Layout (HTML-структура с компонентами ввода и вывода) и Callbacks (Python-функции, связанные с компонентами через их id). Когда пользователь взаимодействует с Input-компонентом (выбирает дату, переключает тумблер), вызывается callback-функция, которая получает значения входов, выполняет вычисления и возвращает результат в Output-компонент (например, обновлённый график).

**11. Зачем указывать `dtype` при чтении CSV с Dask cuDF?**

Если не указать типы данных, cuDF будет их выводить по выборке строк. Это может привести к ошибкам, если редкие значения имеют другой тип (например, float среди целых чисел). Также использование типов меньшей разрядности (float32 вместо float64) уменьшает потребление памяти и ускоряет вычисления. Параметр `dtype` явно задаёт типы и предотвращает подобные проблемы.

**12. Что такое `persist()` в Dask и чем оно отличается от `compute()`?**

`.compute()` выполняет граф вычислений и возвращает результат как обычный объект (pandas DataFrame или cuDF DataFrame) в локальную память. `.persist()` тоже выполняет вычисления, но сохраняет результат распределённым в памяти Dask-кластера. Это полезно после загрузки данных с диска — последующие операции будут значительно быстрее, так как не нужно перечитывать файлы.

**13. Как nvidia-smi помогает при разработке GPU-пайплайнов?**

`nvidia-smi` показывает текущее состояние GPU: использование памяти, загрузку, температуру, энергопотребление. С флагом `--query-gpu` можно выбрать конкретные метрики и вывести в формате CSV. В связке с `watch -n0.1` команда обновляется каждые 0.1 секунды, позволяя в реальном времени отслеживать, как код загружает GPU и расходует память.
